Canada has failed to meaningfully consider the extent that social media “bots” are shaping the country’s political discourse, according to two professors whose research suggests that the software has been used in Canada as far back as 2012. “We need to have more critical awareness of the fact that social media isn’t a mirror of reality – it’s a distorted mirror,” said Fenwick McKelvey, an assistant professor at Montreal’s Concordia University. “And we need to be able to learn how to make sense of these distortions.” He and his colleague Elizabeth Dubois are calling for a code of conduct that would require political parties to reveal when they are using bots – and for what purposes. Strictly defined, a Twitter bot is any automated account on the social network. That can be something as simple as automatically tweeting links to news articles – most of the Guardian's social media accounts are technically Twitter bots, for instance – to complex interactions like automatically generating Emoji-based art or automatically replying to climate change deniers with scientific evidence. But, as with "troll" and "fake news", the strict definition has been forgotten as the term has become one of political conflict. The core of the debate is the accusation that a number of political tweets were sent by "Russian bots", with the intention of subverting political debate, or simply creating chaos generally. Based on what we know about Russian information warfare, the Twitter accounts run by the country's "troll army", based in a nondescript office building in St Petersburg, are unlikely to be automated at all. Instead, accounts like @SouthLoneStar, which pretended to be a Texan right-winger, were probably run by individuals paid 45-65,000 rubles a month to sow discord in Western politics. In other ways, they resembled bots – hence the confusion. They rarely tweeted about themselves, sent far more posts than a typical user, and were single-minded in what they shared. People behaving like bots pretending to be people: this is the nature of modern propaganda. Doing so, they argue, would be a critical step towards shedding light on the role of automation in Canadian politics, in which bots – sometimes posing as real people – have been used to amplify the perceived popularity of politicians, dampen online political discussions and sway public opinion. Their most recent example was during Ontario’s Progressive Conservative leadership race last month. Doug Ford, the brother of the late Toronto mayor Rob Ford, emerged victorious after a campaign that involved a handful of suspicious Twitter accounts that promoted Ford and slandered one of his rivals. McKelvey and Dubois tracked bots dating as far back as 2012, when a follower of the nascent Coalition Avenir Québec created a bot that sent out 11,000 tweets in support of the provincial party. During the 2015 federal election, a professor from the University of British Columbia was targeted by a torrent of automated attacks after he criticised the then prime minister, Stephen Harper, on Twitter. Research suggests that the impact of these bots on Canadian politics has so far been minimal, unlike south of the border, where evidence on the use of bots was recently turned over to congressional investigators looking into Moscow’s interference with the US election campaign. But downplaying their presence in Canada risks trivialising what could be a critical issue for Canadian democracy, said Dubois, an assistant professor at the University of Ottawa. Unlike the Russian bots used to promote Brexit and Donald Trump, bots being used in Canada do not appear to have been created outside of the country, Dubois said. But she highlighted the growing tendency to conflate social media trends with public sentiment. “We need to be careful of taking those stats on popularity or the way an issue is presented in the social media conversation at face value, because it’s very possible – and very likely in fact – that automation and political bots are playing a role.” Much of what is known about bots in Canada comes from Twitter, offering a glimpse of a phenomenon that could be playing out across social media platforms, said McKelvey. A 2017 Reuters Digital News report suggested that 40% of Canadians access news on Facebook, followed by 17% on YouTube and just 11% on Twitter. “Our ability of identifying Twitter bots is merely a way of gesturing to what we don’t know,” said McKelvey. “It’s a way of trying to call for greater accountability of these other platforms, where spam and other forms of influencer marketing or media manipulation might be just as – if not more – prominent. But we as researchers can’t know or make claims about that.” Their research has also documented the many ways in which bots can be used to heighten the democratic process, such as those created by political parties to disseminate information about policies being debated, issues or candidates. Others are aimed at providing transparency, such as the @gccaedits bot, which sends out a tweet each time an anonymous edit is made to Wikipedia from a Government of Canada IP address. In the recent leadership race for Ontario’s conservatives, the suspicious Twitter accounts were relatively easy to spot – many of them were created within minutes of each other, used photos found online and employed similar language. This ease of detection is one advantage of tackling the issue now, said Dubois, as the technology behind bots will probably improve in the coming years. “And when it’s more difficult to identify these bots … it’s really easy to think that the topics people care about on social media are in fact the ones that the general public cares about,” said Dubois, “and that the perspective that they’re putting forward is the perspective that most Canadians would have – when it really isn’t.”