Google knows you’re pregnant. Spotify knows your favorite throwback jams. Is this convenient or creepy? It depends. One minute, you’re grateful for the personalized precision of Netflix’s recommendations. The next, you’re nauseated by the personalized precision of a Facebook ad. Big data has been around for awhile, but our discomfort with it is relatively recent. The election of Donald Trump punctured many powerful fictions, among them the belief in the beneficence of the tech industry. There is now greater public awareness of how a handful of large companies use technology to monitor and manipulate us. This awareness is a wonderful thing. But if we want to channel the bad feelings swirling around tech into something more enduring, we need to radicalize the conversation. It’s good that more people see a problem where they didn’t before. The next step is showing them that the problem is larger than they think. Big data is not confined to the cluster of companies that we know, somewhat imprecisely, as the tech industry. Rather, it describes a particular way of acquiring and organizing information that is increasingly indispensable to the economy as a whole. When you think about big data, you shouldn’t just think about Google and Facebook; you should think about manufacturing and retail and logistics and healthcare. With digitization, capitalism starts to eat reality itself. It begins to consume moments Understanding big data, then, is crucial for understanding what capitalism currently is and what it is becoming – and how we might transform it. Rosa Luxemburg once observed that capitalism grows by consuming anything that isn’t capitalist. It eats the world, to adapt Silicon Valley investor Marc Andreessen’s famous phrase. Historically, this has often involved literal imperialism: a developed country uses force against an undeveloped one in order to extract raw materials, exploit cheap labor and create markets. With digitization, however, capitalism starts to eat reality itself. It becomes an imperialism of everyday life – it begins to consume moments. In the classic science-fiction film The Blob, a meteorite lands in a small town carrying an alien amoeba. The amoeba starts expanding, swallowing up people and structures, threatening to envelop the whole town, until the air force swoops in and airlifts it to the Arctic. Big data will eventually become so big that it devours everything. One way to respond is to try to kill it – to rip out the Blob and dump it in the Arctic. That seems to be what a certain school of technology critics want. Writers such as Franklin Foer denounce digitization as a threat to our essential humanity, while tech industry “refuseniks” warn us about the damaging psychological effects of the technologies they helped create. This is the path of retreat from the digital, towards the “authentically human” – an idea that’s generally associated with reading more books and having more face-to-face conversations. The other route is to build a better Blob. Data is the new oil, says everyone. The analogy has become something of a cliche, widely deployed in media coverage of the digital economy. But it’s a useful comparison – more useful, in fact, than people realize. Because thinking of data as a resource like oil helps illuminate not only how it functions, but how we might organize it differently. Big data is extractive. It involves extracting data from various “mines” – Facebook, say, or a connected piece of industrial equipment. This raw material must then be “refined” into potentially valuable knowledge by combining it with other data and analyzing it. Extractive industries need to be closely regulated because they generate all sorts of externalities – costs that aren’t borne by the company, but are instead passed on to society as a whole. There are certain kinds of resources that we shouldn’t be extracting at all, because those costs are far too high, like fossil fuels. There are others that we should only be extracting under very specific conditions, with adequate protections for workers, the environment, and the broader public. And democratic participation is crucial: you shouldn’t build a mine in a community that doesn’t want it. These principles offer a framework for governing big data. There are certain kinds of data we shouldn’t be extracting. There are certain places where we shouldn’t build data mines. And the incredibly complex and opaque process whereby raw data is refined into knowledge needs to be cracked wide open, so we can figure out what further rules are required. Like any extractive endeavor, big data produces externalities. The extractors reap profits, while the rest of us are left with the personal, social and environmental consequences. These range from the annihilation of privacy to algorithmic racism to a rapidly warming climate. The world’s data centers, for instance, put about as much carbon into the atmosphere as air travel. Society, not industry, should decide how and where resources are extracted. Big data is no different. Regulating big data is a good start, but it’s far from revolutionary. To democratize big data, we need to change who benefits from its use. Under the current model, data is owned largely by big companies and used for profit. Under a more democratic model, what would it look like instead? Again, the oil metaphor is useful. Developing countries have often embraced “resource nationalism”, the idea that a state should control the resources found within its borders, not foreign corporations. A famous example is Mexico: in 1938, the president, Lázaro Cárdenas, nationalized the country’s oil reserves and expropriated the equipment of foreign-owned oil companies. “The oil is ours!” Mexicans cheered. Data is no less a form of common property than oil or soil or copper. We make data together, and we make it meaningful together, but its value is currently captured by the companies that own it. We find ourselves in the position of a colonized country, our resources extracted to fill faraway pockets. Wealth that belongs to the many – wealth that could help feed, educate, house and heal people – is used to enrich the few. The solution is to take up the template of resource nationalism, and nationalize our data reserves. This isn’t as abstract as it sounds. It would begin with the recognition that all of the data extracted within a country is the common property of everyone who lives in that country. Such a move wouldn’t necessarily require seizing the extractive apparatus itself. You don’t have to nationalize the data centers to nationalize the data. Companies could continue to extract and refine data, but with the crucial distinction that they are doing so for our benefit. In the oil industry, companies often sign “production sharing agreements” (PSAs) with governments. The government hires the company as a contractor to explore, develop, and produce the oil, but retains ownership of the oil itself. The company bears the cost and risk of the venture, and in exchange receives a portion of the revenue. The rest goes to the government. Bringing data revenues into public coffers is only the first step, however. We also need to distribute those revenues as widely as possible. In 1976, Alaska established a sovereign wealth fund with a share of the rents and royalties collected from oil companies drilling on state lands. Since 1982, the fund has paid out an annual dividend to every Alaskan citizen. The exact amount fluctuates with the fund’s performance, but in the last few years, it’s generally ranged from $1,000 to $2,000. A data fund that distributes a data dividend would help democratize big data We could do the same with data. In exchange for permission to extract and refine our data, companies would be required to pay a certain percentage of their data revenue into a sovereign wealth fund. The fund could use that capital to acquire other income-producing assets, as the Alaskan fund has, and pay out an annual dividend to all citizens. If it were generous enough, this dividend could even function as a universal basic income, along the lines of what the social commentator Matt Bruenig has proposed. A data fund that distributes a data dividend would help democratize big data. It would enable us to collectively benefit from a resource we collectively create. It would transform data from a private asset stockpiled by corporations to make a small number of people rich into a form of social property held in common by everyone who helps create it. Technology helps set the parameters of possibility. It frames our range of potential futures, but it doesn’t select one for us. The potential futures framed by big data have a particularly wide range: they run from the somewhat annoying to the very miserable, from the reasonably humane to the delightfully utopian. Where we land in this grid will come down to who owns the machines, and how they’re used – a matter for power, and politics, to decide. A longer version of this piece appears in the upcoming “Scale” issue of Logic, a magazine about technology. Visit logicmag.io to learn more.