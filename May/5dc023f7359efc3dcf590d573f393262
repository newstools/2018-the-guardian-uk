Spoiler alert: this article discusses plot points from season one of the HBO series Westworld. Buried inside science fictional visions of future societies – filled with robots and aliens – lie questions about our own lives. From Star Trek to Tarkovsky, HG Wells to Ursula K Le Guin, all asked what it means to be a human – how do we treat life forms deemed to be “different” and how else might we order our own societies? Westworld, which has just entered its second season, is no stranger to this trope. While the story – developed from the 1973 film written and directed by Michael Crichton – features a futuristic, immersive wild west theme park populated by lifelike robots willing to indulge clients’ every fantasy, the ethical question raised by the series is profound: what it is that defines the human experience. The answers it offers are extremely dark. The series is marked by its relentlessly negative depiction of humans let off the leash. Unrestricted by laws prohibiting crimes against the person, the holidaymakers of the vast plains of Westworld spend their time indulging in orgies of violence by stabbing and shooting the robotic “hosts”. The series pulls few punches; the gunshots leave real wounds, the crushed heads really cave in and the women of Westworld are exposed to relentless sexual violence at the hands of the male guests. While the guests are indulgently anti-robot, does the show itself revel in being indulgently anti-human? The 3D printed bodies that the robotic hosts inhabit are a clear visual reference to the Vitruvian Man, Leonardo da Vinci’s diagram of the ideal human, an iconic image from the Renaissance and the birth of humanist ideals. Like the avuncular John Hammond, the director of Crichton’s later dystopian world Jurassic Park, Anthony Hopkins’ Robert Ford, the director of Westworld, seems driven by a similarly enlightenment-inspired humanistic faith in the ability of scientific progress to illuminate – in this case, by offering guests the ability to explore their own sense of self through his storylines. Ford seems worryingly unconcerned that he is catering to his guest’s predilections for inflicting savage violence and sexual assault upon his simulacra humans. Their own human barbarity is far from a moral quandary or social evil to be addressed; it is simply a market for his entertainment to exploit and a useful plot device to serve his own obsession with storytelling. It’s telling that Ford sets his theme park in the US wild west, itself a place where a confected narrative of “manifest destiny” used European-developed humanistic ideals of scientific progress and the taming of nature to develop a dehumanising racial science and perpetrate genocide against the Native Americans who inhabited the land. Those ruled “non-human” are, at most, resources. Few human characters in the show even approach likability; indeed, as the first series progresses, one of the only staff members in Delos Destinations, the organisation that runs Westworld, to struggle with the ethical implications of his job is later revealed to be a robot. The moral conundrums he wrestles with are a function of his “becoming-conscious”. Almost without exception, the scientists and programmers who work within the vast capitalist conglomerate of Delos are avaricious and sneaky. Their world is organised by a hierarchy of power that rewards all the worst human characteristics, so long as they benefit the corporation. The guests they serve live as mindless consumers whose desires and traumas are not to be questioned, but indulged and exploited. They, too, are shaped by the nature and organisation of the society they’re in; one with a market in crypto-murder. The uncertain, shifting time frames of the show allow the only “good guy” to be revealed as the youthful incarnation of the lawless frontier’s most villainous ne’er-do-well, a sadist played by Ed Harris who indulges in rape and butchery in his obsessive quest to uncover what he believes is the park’s true secret. It’s a life exposed to simulated violence that has shaped him into the cruel “Man in Black”, driven by his own selfish desires. The questions that Westworld asks about what being human means are instead posed through the hosts. Already conscious and semi-sentient, they have many of the characteristics of true artificial intelligence themselves, long since having managed to pass the famous Turing test that makes them functionally indistinguishable from humans. But after each completed “storyline” in the park, their memories are wiped. The sci-fi morality tale lies in the robots’ gradual rebellion, as they become dangerously human only when their programming fails and they begin to experience memory – and with it, psychological trauma. Scientific progress facilitates both hubristic megalomania and brutal cruelty, the show seems to tell us, but it is trauma and a concomitant desire for both revenge and liberation that lies at the core of what makes us truly human.